{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " i202208_Haseeb_Waheed_Khan_MLDS_A.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "E-919K-jy9AZ",
        "outputId": "d2128aba-8b58-4ea5-b96b-312edc0922a0"
      },
      "source": [
        "# ! pip install hummingbird\n",
        "# ! pip install hummingbird-ml[extra]\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "tf.test.is_built_with_cuda()\n",
        "print(tf.version.VERSION)\n",
        "import sys\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "S8gDnPtqiKSc",
        "outputId": "5974766a-3791-4344-dab0-3ca795c544ab"
      },
      "source": [
        "\n",
        "# importing required libraries\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning\"\n",
        "os.chdir(path)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Machine Learning/Assignment_2_ML'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZE1Rya5iXTM"
      },
      "source": [
        "## Read csv File \n",
        "def read_file():\n",
        "    Xtr=np.loadtxt(\"TrainData.csv\")\n",
        "    Ytr=np.loadtxt(\"TrainLabels.csv\") \n",
        "    Xts=np.loadtxt(\"TestData.csv\")\n",
        "    print(Xtr.shape)\n",
        "    return Xtr, Ytr, Xts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPcXMegEjs1g"
      },
      "source": [
        "\n",
        "def extractfeatures(input_vector):\n",
        "    '''\n",
        "    Extracting image features using histogram of oriented gradients\n",
        "    \n",
        "    Arguments:\n",
        "    Numpy vector array \n",
        "\n",
        "    Returns:\n",
        "    Feature vector\n",
        "    '''\n",
        "    img = np.reshape(input_vector,(28,28))\n",
        "\n",
        "    # resizing image standard image dimention = 128:64 \n",
        "    resized_img = resize(img, (128,64)) \n",
        "\n",
        "    # creating hog features \n",
        "    fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "    \n",
        "    # resizing image to a vector\n",
        "    hog_image_vector = hog_image.reshape(128*64)\n",
        "    return hog_image_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBb7Qu4VJ1Ay"
      },
      "source": [
        "def FeatureMatrix(Xtr, Ytr, Xts):\n",
        "    # x_train = []\n",
        "    # for index in range(len(Xtr)):\n",
        "    #     x_train.append(extractfeatures(Xtr[index]))\n",
        "    # Xtr_train = np.array(x_train)\n",
        "\n",
        "    x_feature = np.array([extractfeatures(Xtr[index]) for index in range(len(Xtr))])\n",
        "    return (x_feature, Ytr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87EUcd2Nl21c"
      },
      "source": [
        "class LogisticRegression:\n",
        "    '''\n",
        "    Logistic regression technique used for binary classification\n",
        "    \n",
        "    Arguments :\n",
        "    Xt_train = Numpy matrix with dimention (no_of_sample x  no_of_features)\n",
        "    Ytr_train = Numpy vector array with dimention (no_of_sample x 1)\n",
        "\n",
        "    Return:\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self,Xtr_train,Ytr_train,pen, lr_c = 1):\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        \n",
        "        self.x_train = Xtr_train\n",
        "        self.y_train = Ytr_train\n",
        "        pen = 'l2'\n",
        "        if (pen == 'l2' or pen == 'none'):\n",
        "            # all parameters not specified are set to their defaults\n",
        "            self.logisticRegr = LogisticRegression(penalty = pen, max_iter=12002,C = lr_c)\n",
        "        elif pen == 'l1':\n",
        "             # all parameters not specified are set to their defaults\n",
        "            self.logisticRegr = LogisticRegression(penalty = pen, max_iter=12002, solver='lbfgs') \n",
        "\n",
        "\n",
        "        # logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
        "        self.logisticRegr.fit(self.x_train, self.y_train)\n",
        "\n",
        "\n",
        "    def PredictLable(self, test_matrix):\n",
        "        '''\n",
        "        Predict output of testing matrix after model training\n",
        "\n",
        "        Argument:\n",
        "        n feature Matrix \n",
        "\n",
        "        Return:       \n",
        "        list of a predicted values in the for of a Vector (m x 1)\n",
        "        '''\n",
        "        return self.logisticRegr.predict(test_matrix.reshape(1,-1))\n",
        "\n",
        "\n",
        "    def CalculateModelAccuracy(self,Xt_test,Yt_test):\n",
        "        X_test = Xt_test\n",
        "        Y_test = Yt_test\n",
        "        return self.logisticRegr.score(X_test,Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZxDGfqiI2Xa"
      },
      "source": [
        "class SupportVectorMachine():\n",
        "    \n",
        "    def TrainModel(self,Xtr_train, Ytr_train, svm_kernel,svm_c = 1):\n",
        "        import numpy as np\n",
        "        from sklearn import svm\n",
        "\n",
        "\n",
        "        self.Xtr_train = Xtr_train\n",
        "        self.Ytr_train = Ytr_train\n",
        "        # Fit the SVM model\n",
        "        self.model = svm.SVC(kernel = svm_kernel,max_iter=10000, C = svm_c)\n",
        "        self.model.fit(self.Xtr_train, self.Ytr_train)\n",
        "        \n",
        "\n",
        "    def PredictValues(self,test_vector):\n",
        "        pred_value = test_vector.reshape(1, -1)\n",
        "        return self.model.predict(pred_value)\n",
        "    \n",
        "    def CalculateModelAccuracy(self,test_sample,test_lable):\n",
        "        \n",
        "        test_samp = test_sample\n",
        "        test_lab = test_lable \n",
        "        return self.model.score(test_samp,test_lab)\n",
        "\n",
        "    def ConfusionMatrix(self,test_sample,test_lable):\n",
        "        from sklearn.metrics import classification_report\n",
        "        test_samp = test_sample\n",
        "        test_lab = test_lable\n",
        "        y_true = [int(self.PredictValues(test_sample[i])) for i in range(len(test_sample))]\n",
        "        y_pred = [int(test_lab[i]) for i in range(len(test_sample))]\n",
        "        target_names = ['True', 'False']\n",
        "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "        \n",
        "    \n",
        "        accuracy_score(y_true, y_pred, normalize=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2GcdNOLz_8S"
      },
      "source": [
        "def GetScore(model_name,Xtr_train, Xt_test, Ytr_train, Yt_test, svm_kernel, svm_c,lr_pen):\n",
        "\n",
        "    \n",
        "    X_train = Xtr_train\n",
        "    Y_train  = Ytr_train\n",
        "    Y_test = Yt_test\n",
        "    X_test = Xt_test \n",
        "    name = model_name\n",
        "    if name == \"SVM\":\n",
        "        s = SupportVectorMachine()\n",
        "        s.TrainModel(X_train, Y_train, svm_kernel, svm_c)\n",
        "        # s.ConfusionMatrix(X_train,Y_train) \n",
        "        return s.CalculateModelAccuracy(X_test,Y_test)\n",
        "    \n",
        "    elif name == \"LR\" :\n",
        "\n",
        "        lreg = LogisticRegression(X_train,Y_train,lr_pen, svm_c) #  regularization parameter = None means no regularization\n",
        "        return lreg.CalculateModelAccuracy(X_test,Y_test)   # Finding accuracy of model\n",
        "      \n",
        "         \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL7NTyz6XuOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e301be-0ad8-4199-c140-411109a6750a"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#Read file \n",
        "(Xtr, Ytr, Xts) = read_file()\n",
        "# Extracting Features\n",
        "(Xtr_train, Ytr_train) = FeatureMatrix(Xtr, Ytr, Xts) # For Testing \n",
        "# (Xtr_train, Ytr_train) = FeatureMatrix(Xtr, Ytr, Xts)\n",
        "\n",
        "\n",
        "Xtr_train= Xtr_train1\n",
        "Ytr_train = Ytr_train1\n",
        "\n",
        "svm_model_accuracy = {}\n",
        "lr_model_accuracy = {}\n",
        "folds = StratifiedKFold(n_splits = 5)\n",
        "svm_kernel = 'rbf' \n",
        "lr_pen = 'l2'\n",
        "svm_c_vector = [100, 50, 10, 5, 1, 0.1, 0.01, 0.001, 0.0001]  # Regularization Parameter\n",
        "i = 1\n",
        "for svm_c in svm_c_vector:\n",
        "    score_svm = []\n",
        "    score_lr  = []\n",
        "    for train_index, test_index in folds.split(Xtr_train, Ytr_train):\n",
        "        X_train, X_test, Y_train, Y_test = Xtr_train[train_index], Xtr_train[test_index], Ytr_train[train_index], Ytr_train[test_index]\n",
        "        \n",
        "        score_lr.append(GetScore(\"LR\",X_train, X_test, Y_train, Y_test, svm_kernel, svm_c,lr_pen))\n",
        "        score_svm.append(GetScore(\"SVM\",X_train, X_test, Y_train, Y_test, svm_kernel, svm_c, lr_pen))\n",
        "        # print(f\"For K = {i} \\nSVM Accuracy = {score_svm[-1]}\\nLR  Accuracy = {score_lr[-1]}\")\n",
        "        i += 1\n",
        "\n",
        "    # In LR smaller values of C = stronger regularization\n",
        "    svm_model_accuracy[svm_c] =  np.sum(score_svm) / 5  \n",
        "    lr_model_accuracy[svm_c] =  np.sum(score_lr) / 5    \n",
        "    print(f\"\\n_________________\\n\\n5-fold cross-validation\\nFor Regularization Parameter = {svm_c}:\\n\\nSVM Accuraccy = {score_svm} \\nTotal =  {round(svm_model_accuracy[svm_c]*100,3)}%\\n\\nLR Accuraccy = {score_lr} \\nTotal = {round(lr_model_accuracy[svm_c]*100,3)}%\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMJKvdzXPSv"
      },
      "source": [
        "# Choosing the best hyper parameter and model\n",
        "## Model Trainig \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBhSbVAT9TJ"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#Read file \n",
        "(Xtr, Ytr, Xts) = read_file()\n",
        "# Extracting Features\n",
        "(Xtr_train, Ytr_train) = FeatureMatrix(Xtr, Ytr, Xts) # For Testing \n",
        "# (Xtr_train, Ytr_train) = FeatureMatrix(Xtr, Ytr, Xts)\n",
        "\n",
        "Xtr_train= Xtr_train1\n",
        "Ytr_train = Ytr_train1\n",
        "\n",
        "svm_model_accuracy = {}\n",
        "lr_model_accuracy = {}\n",
        "folds = StratifiedKFold(n_splits = 5)\n",
        "svm_kernel = 'rbf' \n",
        "lr_pen = 'l2'\n",
        "svm_c = 1 # Regularization Parameter\n",
        "i = 1\n",
        "\n",
        "s = SupportVectorMachine()\n",
        "\n",
        "for train_index, test_index in folds.split(Xtr_train, Ytr_train):\n",
        "    X_train, X_test, Y_train, Y_test = Xtr_train[train_index], Xtr_train[test_index], Ytr_train[train_index], Ytr_train[test_index]\n",
        "\n",
        "    s.TrainModel(X_train, Y_train, svm_kernel, svm_c)\n",
        "      \n",
        "\n",
        "# Save the model as a pickle in a file\n",
        "from sklearn.externals import joblib \n",
        "joblib.dump(s, 'myModel.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn0ARFf2ZTrw"
      },
      "source": [
        "# #Read file \n",
        "# (Xtr, Ytr, Xts) = read_file()\n",
        "# (Xts_train, Ytr_train) = FeatureMatrix(Xts[0:len(Xts)], Ytr[0:300], Xts[0:len(Xts)]) # For Testing \n",
        "\n",
        "Yts = [(s.PredictValues(sample)) for sample in Xts_train]\n",
        "print(Yts)\n",
        "np.savetxt(\"myPredictions.csv\", Yts, newline='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DleqAmLGi-F-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}